{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You must supply the dataset directory with --dataset_dir",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fbfd5f60eb20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m   \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m    123\u001b[0m   \u001b[1;31m# Call the main function, passing through any arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m   \u001b[1;31m# to the final program.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m   \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-fbfd5f60eb20>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'You must supply the dataset directory with --dataset_dir'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m   \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You must supply the dataset directory with --dataset_dir"
     ]
    }
   ],
   "source": [
    "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Generic training script that trains a model using a given dataset.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from datasets import dataset_factory\n",
    "from deployment import model_deploy\n",
    "from nets import nets_factory\n",
    "from preprocessing import preprocessing_factory\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'master', '', 'The address of the TensorFlow master to use.')\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'train_dir', '/tmp/tfmodel/',\n",
    "    'Directory where checkpoints and event logs are written to.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('num_clones', 1,\n",
    "                            'Number of model clones to deploy.')\n",
    "\n",
    "tf.app.flags.DEFINE_boolean('clone_on_cpu', False,\n",
    "                            'Use CPUs to deploy clones.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('worker_replicas', 1, 'Number of worker replicas.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'num_ps_tasks', 0,\n",
    "    'The number of parameter servers. If the value is 0, then the parameters '\n",
    "    'are handled locally by the worker.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'num_readers', 4,\n",
    "    'The number of parallel readers that read data from the dataset.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'num_preprocessing_threads', 4,\n",
    "    'The number of threads used to create the batches.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'log_every_n_steps', 10,\n",
    "    'The frequency with which logs are print.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'save_summaries_secs', 600,\n",
    "    'The frequency with which summaries are saved, in seconds.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'save_interval_secs', 600,\n",
    "    'The frequency with which the model is saved, in seconds.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'task', 0, 'Task id of the replica running the training.')\n",
    "\n",
    "######################\n",
    "# Optimization Flags #\n",
    "######################\n",
    "\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'weight_decay', 0.00004, 'The weight decay on the model weights.')\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'optimizer', 'rmsprop',\n",
    "    'The name of the optimizer, one of \"adadelta\", \"adagrad\", \"adam\",'\n",
    "    '\"ftrl\", \"momentum\", \"sgd\" or \"rmsprop\".')\n",
    "\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'adadelta_rho', 0.95,\n",
    "    'The decay rate for adadelta.')\n",
    "\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'adagrad_initial_accumulator_value', 0.1,\n",
    "    'Starting value for the AdaGrad accumulators.')\n",
    "\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'adam_beta1', 0.9,\n",
    "    'The exponential decay rate for the 1st moment estimates.')\n",
    "\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'adam_beta2', 0.999,\n",
    "    'The exponential decay rate for the 2nd moment estimates.')\n",
    "\n",
    "tf.app.flags.DEFINE_float('opt_epsilon', 1.0, 'Epsilon term for the optimizer.')\n",
    "\n",
    "tf.app.flags.DEFINE_float('ftrl_learning_rate_power', -0.5,\n",
    "                          'The learning rate power.')\n",
    "\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'ftrl_initial_accumulator_value', 0.1,\n",
    "    'Starting value for the FTRL accumulators.')\n",
    "\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'ftrl_l1', 0.0, 'The FTRL l1 regularization strength.')\n",
    "\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'ftrl_l2', 0.0, 'The FTRL l2 regularization strength.')\n",
    "\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'momentum', 0.9,\n",
    "    'The momentum for the MomentumOptimizer and RMSPropOptimizer.')\n",
    "\n",
    "tf.app.flags.DEFINE_float('rmsprop_momentum', 0.9, 'Momentum.')\n",
    "\n",
    "tf.app.flags.DEFINE_float('rmsprop_decay', 0.9, 'Decay term for RMSProp.')\n",
    "\n",
    "#######################\n",
    "# Learning Rate Flags #\n",
    "#######################\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'learning_rate_decay_type',\n",
    "    'exponential',\n",
    "    'Specifies how the learning rate is decayed. One of \"fixed\", \"exponential\",'\n",
    "    ' or \"polynomial\"')\n",
    "\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'end_learning_rate', 0.0001,\n",
    "    'The minimal end learning rate used by a polynomial decay learning rate.')\n",
    "\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'label_smoothing', 0.0, 'The amount of label smoothing.')\n",
    "\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'learning_rate_decay_factor', 0.94, 'Learning rate decay factor.')\n",
    "\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'num_epochs_per_decay', 2.0,\n",
    "    'Number of epochs after which learning rate decays.')\n",
    "\n",
    "tf.app.flags.DEFINE_bool(\n",
    "    'sync_replicas', False,\n",
    "    'Whether or not to synchronize the replicas during training.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'replicas_to_aggregate', 1,\n",
    "    'The Number of gradients to collect before updating params.')\n",
    "\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'moving_average_decay', None,\n",
    "    'The decay to use for the moving average.'\n",
    "    'If left as None, then moving averages are not used.')\n",
    "\n",
    "#######################\n",
    "# Dataset Flags #\n",
    "#######################\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'dataset_name', 'imagenet', 'The name of the dataset to load.')\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'dataset_split_name', 'train', 'The name of the train/test split.')\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'dataset_dir', None, 'The directory where the dataset files are stored.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'labels_offset', 0,\n",
    "    'An offset for the labels in the dataset. This flag is primarily used to '\n",
    "    'evaluate the VGG and ResNet architectures which do not use a background '\n",
    "    'class for the ImageNet dataset.')\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'model_name', 'inception_v3', 'The name of the architecture to train.')\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'preprocessing_name', None, 'The name of the preprocessing to use. If left '\n",
    "    'as `None`, then the model_name flag is used.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'batch_size', 32, 'The number of samples in each batch.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'train_image_size', None, 'Train image size')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('max_number_of_steps', None,\n",
    "                            'The maximum number of training steps.')\n",
    "\n",
    "#####################\n",
    "# Fine-Tuning Flags #\n",
    "#####################\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'checkpoint_path', None,\n",
    "    'The path to a checkpoint from which to fine-tune.')\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'checkpoint_exclude_scopes', None,\n",
    "    'Comma-separated list of scopes of variables to exclude when restoring '\n",
    "    'from a checkpoint.')\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'trainable_scopes', None,\n",
    "    'Comma-separated list of scopes to filter the set of variables to train.'\n",
    "    'By default, None would train all the variables.')\n",
    "\n",
    "tf.app.flags.DEFINE_boolean(\n",
    "    'ignore_missing_vars', False,\n",
    "    'When restoring a checkpoint would ignore missing variables.')\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "\n",
    "def _configure_learning_rate(num_samples_per_epoch, global_step):\n",
    "  \"\"\"Configures the learning rate.\n",
    "\n",
    "  Args:\n",
    "    num_samples_per_epoch: The number of samples in each epoch of training.\n",
    "    global_step: The global_step tensor.\n",
    "\n",
    "  Returns:\n",
    "    A `Tensor` representing the learning rate.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if\n",
    "  \"\"\"\n",
    "  decay_steps = int(num_samples_per_epoch / FLAGS.batch_size *\n",
    "                    FLAGS.num_epochs_per_decay)\n",
    "  if FLAGS.sync_replicas:\n",
    "    decay_steps /= FLAGS.replicas_to_aggregate\n",
    "\n",
    "  if FLAGS.learning_rate_decay_type == 'exponential':\n",
    "    return tf.train.exponential_decay(FLAGS.learning_rate,\n",
    "                                      global_step,\n",
    "                                      decay_steps,\n",
    "                                      FLAGS.learning_rate_decay_factor,\n",
    "                                      staircase=True,\n",
    "                                      name='exponential_decay_learning_rate')\n",
    "  elif FLAGS.learning_rate_decay_type == 'fixed':\n",
    "    return tf.constant(FLAGS.learning_rate, name='fixed_learning_rate')\n",
    "  elif FLAGS.learning_rate_decay_type == 'polynomial':\n",
    "    return tf.train.polynomial_decay(FLAGS.learning_rate,\n",
    "                                     global_step,\n",
    "                                     decay_steps,\n",
    "                                     FLAGS.end_learning_rate,\n",
    "                                     power=1.0,\n",
    "                                     cycle=False,\n",
    "                                     name='polynomial_decay_learning_rate')\n",
    "  else:\n",
    "    raise ValueError('learning_rate_decay_type [%s] was not recognized',\n",
    "                     FLAGS.learning_rate_decay_type)\n",
    "\n",
    "\n",
    "def _configure_optimizer(learning_rate):\n",
    "  \"\"\"Configures the optimizer used for training.\n",
    "\n",
    "  Args:\n",
    "    learning_rate: A scalar or `Tensor` learning rate.\n",
    "\n",
    "  Returns:\n",
    "    An instance of an optimizer.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if FLAGS.optimizer is not recognized.\n",
    "  \"\"\"\n",
    "  if FLAGS.optimizer == 'adadelta':\n",
    "    optimizer = tf.train.AdadeltaOptimizer(\n",
    "        learning_rate,\n",
    "        rho=FLAGS.adadelta_rho,\n",
    "        epsilon=FLAGS.opt_epsilon)\n",
    "  elif FLAGS.optimizer == 'adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(\n",
    "        learning_rate,\n",
    "        initial_accumulator_value=FLAGS.adagrad_initial_accumulator_value)\n",
    "  elif FLAGS.optimizer == 'adam':\n",
    "    optimizer = tf.train.AdamOptimizer(\n",
    "        learning_rate,\n",
    "        beta1=FLAGS.adam_beta1,\n",
    "        beta2=FLAGS.adam_beta2,\n",
    "        epsilon=FLAGS.opt_epsilon)\n",
    "  elif FLAGS.optimizer == 'ftrl':\n",
    "    optimizer = tf.train.FtrlOptimizer(\n",
    "        learning_rate,\n",
    "        learning_rate_power=FLAGS.ftrl_learning_rate_power,\n",
    "        initial_accumulator_value=FLAGS.ftrl_initial_accumulator_value,\n",
    "        l1_regularization_strength=FLAGS.ftrl_l1,\n",
    "        l2_regularization_strength=FLAGS.ftrl_l2)\n",
    "  elif FLAGS.optimizer == 'momentum':\n",
    "    optimizer = tf.train.MomentumOptimizer(\n",
    "        learning_rate,\n",
    "        momentum=FLAGS.momentum,\n",
    "        name='Momentum')\n",
    "  elif FLAGS.optimizer == 'rmsprop':\n",
    "    optimizer = tf.train.RMSPropOptimizer(\n",
    "        learning_rate,\n",
    "        decay=FLAGS.rmsprop_decay,\n",
    "        momentum=FLAGS.rmsprop_momentum,\n",
    "        epsilon=FLAGS.opt_epsilon)\n",
    "  elif FLAGS.optimizer == 'sgd':\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  else:\n",
    "    raise ValueError('Optimizer [%s] was not recognized', FLAGS.optimizer)\n",
    "  return optimizer\n",
    "\n",
    "\n",
    "def _get_init_fn():\n",
    "  \"\"\"Returns a function run by the chief worker to warm-start the training.\n",
    "\n",
    "  Note that the init_fn is only run when initializing the model during the very\n",
    "  first global step.\n",
    "\n",
    "  Returns:\n",
    "    An init function run by the supervisor.\n",
    "  \"\"\"\n",
    "  if FLAGS.checkpoint_path is None:\n",
    "    return None\n",
    "\n",
    "  # Warn the user if a checkpoint exists in the train_dir. Then we'll be\n",
    "  # ignoring the checkpoint anyway.\n",
    "  if tf.train.latest_checkpoint(FLAGS.train_dir):\n",
    "    tf.logging.info(\n",
    "        'Ignoring --checkpoint_path because a checkpoint already exists in %s'\n",
    "        % FLAGS.train_dir)\n",
    "    return None\n",
    "\n",
    "  exclusions = []\n",
    "  if FLAGS.checkpoint_exclude_scopes:\n",
    "    exclusions = [scope.strip()\n",
    "                  for scope in FLAGS.checkpoint_exclude_scopes.split(',')]\n",
    "\n",
    "  # TODO(sguada) variables.filter_variables()\n",
    "  variables_to_restore = []\n",
    "  for var in slim.get_model_variables():\n",
    "    excluded = False\n",
    "    for exclusion in exclusions:\n",
    "      if var.op.name.startswith(exclusion):\n",
    "        excluded = True\n",
    "        break\n",
    "    if not excluded:\n",
    "      variables_to_restore.append(var)\n",
    "\n",
    "  if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n",
    "    checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n",
    "  else:\n",
    "    checkpoint_path = FLAGS.checkpoint_path\n",
    "\n",
    "  tf.logging.info('Fine-tuning from %s' % checkpoint_path)\n",
    "\n",
    "  return slim.assign_from_checkpoint_fn(\n",
    "      checkpoint_path,\n",
    "      variables_to_restore,\n",
    "      ignore_missing_vars=FLAGS.ignore_missing_vars)\n",
    "\n",
    "\n",
    "def _get_variables_to_train():\n",
    "  \"\"\"Returns a list of variables to train.\n",
    "\n",
    "  Returns:\n",
    "    A list of variables to train by the optimizer.\n",
    "  \"\"\"\n",
    "  if FLAGS.trainable_scopes is None:\n",
    "    return tf.trainable_variables()\n",
    "  else:\n",
    "    scopes = [scope.strip() for scope in FLAGS.trainable_scopes.split(',')]\n",
    "\n",
    "  variables_to_train = []\n",
    "  for scope in scopes:\n",
    "    variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "    variables_to_train.extend(variables)\n",
    "  return variables_to_train\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  if not FLAGS.dataset_dir:\n",
    "    raise ValueError('You must supply the dataset directory with --dataset_dir')\n",
    "\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "  with tf.Graph().as_default():\n",
    "    #######################\n",
    "    # Config model_deploy #\n",
    "    #######################\n",
    "    deploy_config = model_deploy.DeploymentConfig(\n",
    "        num_clones=FLAGS.num_clones,\n",
    "        clone_on_cpu=FLAGS.clone_on_cpu,\n",
    "        replica_id=FLAGS.task,\n",
    "        num_replicas=FLAGS.worker_replicas,\n",
    "        num_ps_tasks=FLAGS.num_ps_tasks)\n",
    "\n",
    "    # Create global_step\n",
    "    with tf.device(deploy_config.variables_device()):\n",
    "      global_step = slim.create_global_step()\n",
    "\n",
    "    ######################\n",
    "    # Select the dataset #\n",
    "    ######################\n",
    "    dataset = dataset_factory.get_dataset(\n",
    "        FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\n",
    "\n",
    "    ######################\n",
    "    # Select the network #\n",
    "    ######################\n",
    "    network_fn = nets_factory.get_network_fn(\n",
    "        FLAGS.model_name,\n",
    "        num_classes=(dataset.num_classes - FLAGS.labels_offset),\n",
    "        weight_decay=FLAGS.weight_decay,\n",
    "        is_training=True)\n",
    "\n",
    "    #####################################\n",
    "    # Select the preprocessing function #\n",
    "    #####################################\n",
    "    preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\n",
    "    image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n",
    "        preprocessing_name,\n",
    "        is_training=True)\n",
    "\n",
    "    ##############################################################\n",
    "    # Create a dataset provider that loads data from the dataset #\n",
    "    ##############################################################\n",
    "    with tf.device(deploy_config.inputs_device()):\n",
    "      provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "          dataset,\n",
    "          num_readers=FLAGS.num_readers,\n",
    "          common_queue_capacity=20 * FLAGS.batch_size,\n",
    "          common_queue_min=10 * FLAGS.batch_size)\n",
    "      [image, label] = provider.get(['image', 'label'])\n",
    "      label -= FLAGS.labels_offset\n",
    "\n",
    "      train_image_size = FLAGS.train_image_size or network_fn.default_image_size\n",
    "\n",
    "      image = image_preprocessing_fn(image, train_image_size, train_image_size)\n",
    "\n",
    "      images, labels = tf.train.batch(\n",
    "          [image, label],\n",
    "          batch_size=FLAGS.batch_size,\n",
    "          num_threads=FLAGS.num_preprocessing_threads,\n",
    "          capacity=5 * FLAGS.batch_size)\n",
    "      labels = slim.one_hot_encoding(\n",
    "          labels, dataset.num_classes - FLAGS.labels_offset)\n",
    "      batch_queue = slim.prefetch_queue.prefetch_queue(\n",
    "          [images, labels], capacity=2 * deploy_config.num_clones)\n",
    "\n",
    "    ####################\n",
    "    # Define the model #\n",
    "    ####################\n",
    "    def clone_fn(batch_queue):\n",
    "      \"\"\"Allows data parallelism by creating multiple clones of network_fn.\"\"\"\n",
    "      images, labels = batch_queue.dequeue()\n",
    "      logits, end_points = network_fn(images)\n",
    "\n",
    "      #############################\n",
    "      # Specify the loss function #\n",
    "      #############################\n",
    "      if 'AuxLogits' in end_points:\n",
    "        slim.losses.softmax_cross_entropy(\n",
    "            end_points['AuxLogits'], labels,\n",
    "            label_smoothing=FLAGS.label_smoothing, weights=0.4,\n",
    "            scope='aux_loss')\n",
    "      slim.losses.softmax_cross_entropy(\n",
    "          logits, labels, label_smoothing=FLAGS.label_smoothing, weights=1.0)\n",
    "      return end_points\n",
    "\n",
    "    # Gather initial summaries.\n",
    "    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "\n",
    "    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\n",
    "    first_clone_scope = deploy_config.clone_scope(0)\n",
    "    # Gather update_ops from the first clone. These contain, for example,\n",
    "    # the updates for the batch_norm variables created by network_fn.\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)\n",
    "\n",
    "    # Add summaries for end_points.\n",
    "    end_points = clones[0].outputs\n",
    "    for end_point in end_points:\n",
    "      x = end_points[end_point]\n",
    "      summaries.add(tf.summary.histogram('activations/' + end_point, x))\n",
    "      summaries.add(tf.summary.scalar('sparsity/' + end_point,\n",
    "                                      tf.nn.zero_fraction(x)))\n",
    "\n",
    "    # Add summaries for losses.\n",
    "    for loss in tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):\n",
    "      summaries.add(tf.summary.scalar('losses/%s' % loss.op.name, loss))\n",
    "\n",
    "    # Add summaries for variables.\n",
    "    for variable in slim.get_model_variables():\n",
    "      summaries.add(tf.summary.histogram(variable.op.name, variable))\n",
    "\n",
    "    #################################\n",
    "    # Configure the moving averages #\n",
    "    #################################\n",
    "    if FLAGS.moving_average_decay:\n",
    "      moving_average_variables = slim.get_model_variables()\n",
    "      variable_averages = tf.train.ExponentialMovingAverage(\n",
    "          FLAGS.moving_average_decay, global_step)\n",
    "    else:\n",
    "      moving_average_variables, variable_averages = None, None\n",
    "\n",
    "    #########################################\n",
    "    # Configure the optimization procedure. #\n",
    "    #########################################\n",
    "    with tf.device(deploy_config.optimizer_device()):\n",
    "      learning_rate = _configure_learning_rate(dataset.num_samples, global_step)\n",
    "      optimizer = _configure_optimizer(learning_rate)\n",
    "      summaries.add(tf.summary.scalar('learning_rate', learning_rate))\n",
    "\n",
    "    if FLAGS.sync_replicas:\n",
    "      # If sync_replicas is enabled, the averaging will be done in the chief\n",
    "      # queue runner.\n",
    "      optimizer = tf.train.SyncReplicasOptimizer(\n",
    "          opt=optimizer,\n",
    "          replicas_to_aggregate=FLAGS.replicas_to_aggregate,\n",
    "          total_num_replicas=FLAGS.worker_replicas,\n",
    "          variable_averages=variable_averages,\n",
    "          variables_to_average=moving_average_variables)\n",
    "    elif FLAGS.moving_average_decay:\n",
    "      # Update ops executed locally by trainer.\n",
    "      update_ops.append(variable_averages.apply(moving_average_variables))\n",
    "\n",
    "    # Variables to train.\n",
    "    variables_to_train = _get_variables_to_train()\n",
    "\n",
    "    #  and returns a train_tensor and summary_op\n",
    "    total_loss, clones_gradients = model_deploy.optimize_clones(\n",
    "        clones,\n",
    "        optimizer,\n",
    "        var_list=variables_to_train)\n",
    "    # Add total_loss to summary.\n",
    "    summaries.add(tf.summary.scalar('total_loss', total_loss))\n",
    "\n",
    "    # Create gradient updates.\n",
    "    grad_updates = optimizer.apply_gradients(clones_gradients,\n",
    "                                             global_step=global_step)\n",
    "    update_ops.append(grad_updates)\n",
    "\n",
    "    update_op = tf.group(*update_ops)\n",
    "    with tf.control_dependencies([update_op]):\n",
    "      train_tensor = tf.identity(total_loss, name='train_op')\n",
    "\n",
    "    # Add the summaries from the first clone. These contain the summaries\n",
    "    # created by model_fn and either optimize_clones() or _gather_clone_loss().\n",
    "    summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n",
    "                                       first_clone_scope))\n",
    "\n",
    "    # Merge all summaries together.\n",
    "    summary_op = tf.summary.merge(list(summaries), name='summary_op')\n",
    "\n",
    "\n",
    "    ###########################\n",
    "    # Kicks off the training. #\n",
    "    ###########################\n",
    "    slim.learning.train(\n",
    "        train_tensor,\n",
    "        logdir=FLAGS.train_dir,\n",
    "        master=FLAGS.master,\n",
    "        is_chief=(FLAGS.task == 0),\n",
    "        init_fn=_get_init_fn(),\n",
    "        summary_op=summary_op,\n",
    "        number_of_steps=FLAGS.max_number_of_steps,\n",
    "        log_every_n_steps=FLAGS.log_every_n_steps,\n",
    "        save_summaries_secs=FLAGS.save_summaries_secs,\n",
    "        save_interval_secs=FLAGS.save_interval_secs,\n",
    "        sync_optimizer=optimizer if FLAGS.sync_replicas else None)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-738277db00d0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-738277db00d0>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    python train_image_classifier.py --dataset_name=quiz --dataset_dir=/path/to/data --model_name=densenet --train_dir=/path/to/train_ckpt_den --learning_rate=0.1 --optimizer=rmsprop  --batch_size=16 --clone_on_cpu=True\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python train_image_classifier.py --dataset_name=quiz --dataset_dir=/path/to/data --model_name=densenet --train_dir=/path/to/train_ckpt_den --learning_rate=0.1 --optimizer=rmsprop  --batch_size=16 --clone_on_cpu=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train_image_classifier.py --dataset_name=quiz --dataset_dir=/path/to/data --checkpoint_path=/path/to/inception_v4.ckpt --model_name=inception_v4 --checkpoint_exclude_scopes=InceptionV4/Logits,InceptionV4/AuxLogits/Aux_logits --train_dir=/path/to/train_ckpt --learning_rate=0.001 --optimizer=rmsprop  --batch_size=32 --clone_on_cpu=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
